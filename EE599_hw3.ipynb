{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PulVw_da4GML"
      },
      "source": [
        "# HW3 - EE599 Systems for Machine Learning, Fall 2023\n",
        "University of Southern California\n",
        "\n",
        "Instructors: Arash Saifhashemi, Murali Annavaram\n",
        "\n",
        "In this homework assignment, we will ask you to use various methods to implement convolution operation, and then measure and analyze the performance of each method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG26EClp5nEq"
      },
      "source": [
        "## Prepare your Google Drive\n",
        "- Download `ML_Systems_HW3` zip file from GitHub and unzip the it (you may need to rename the unzipped folder).\n",
        "- Upload unzipped folder to ``My Drive`` in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02VQPV6q4Cav",
        "outputId": "b94f4a41-d9ea-4e22-aca9-448d195d9ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/ml-systems-hw3-Junruiethel-master/src')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITQeV_DO755_"
      },
      "source": [
        "## Verify that you are in the correct working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Y1rLNv7434",
        "outputId": "dd9bb38a-9c58-4c70-98c9-bdf86a622043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml-systems-hw3-Junruiethel-master/src\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dioEAKJm6BUN"
      },
      "source": [
        "## Create a folder named `build` under `ML_Systems_HW3/src`, which will be used to store executable files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XemyfZFZ6BUN",
        "outputId": "09f2ad48-9b3a-4b42-ac0c-3f355085c4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘build’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcEAwSWG6wSi",
        "outputId": "2328a0f6-4f58-4b57-c042-e02334a22a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiled q1_conv2d_naive.cc successfully!\n",
            "Compiled q2_conv2d_toeplitz.cc successfully!\n",
            "\u001b[01m\u001b[Kq3_conv2d_toeplitz_transB.cc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kq3_conv2d_toeplitz_transB.cc:103:21:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput\u001b[m\u001b[K’ was not declared in this scope\n",
            "  103 |   printSumOfOutputs(\u001b[01;31m\u001b[Koutput\u001b[m\u001b[K, output_c * output_p * output_q);\n",
            "      |                     \u001b[01;31m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq3_conv2d_toeplitz_transB.cc:103:29:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_c\u001b[m\u001b[K’ was not declared in this scope\n",
            "  103 |   printSumOfOutputs(output, \u001b[01;31m\u001b[Koutput_c\u001b[m\u001b[K * output_p * output_q);\n",
            "      |                             \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq3_conv2d_toeplitz_transB.cc:103:40:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_p\u001b[m\u001b[K’ was not declared in this scope\n",
            "  103 |   printSumOfOutputs(output, output_c * \u001b[01;31m\u001b[Koutput_p\u001b[m\u001b[K * output_q);\n",
            "      |                                        \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq3_conv2d_toeplitz_transB.cc:103:51:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_q\u001b[m\u001b[K’ was not declared in this scope\n",
            "  103 |   printSumOfOutputs(output, output_c * output_p * \u001b[01;31m\u001b[Koutput_q\u001b[m\u001b[K);\n",
            "      |                                                   \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq3_conv2d_toeplitz_transB.cc:109:31:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktemp_output\u001b[m\u001b[K’ was not declared in this scope\n",
            "  109 |   deallocateAlignedFloatArray(\u001b[01;31m\u001b[Ktemp_output\u001b[m\u001b[K);\n",
            "      |                               \u001b[01;31m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "Error compiling q3_conv2d_toeplitz_transB.cc!\n",
            "\u001b[01m\u001b[Kq4_conv2d_toeplitz_avx.cc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kq4_conv2d_toeplitz_avx.cc:102:21:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput\u001b[m\u001b[K’ was not declared in this scope\n",
            "  102 |   printSumOfOutputs(\u001b[01;31m\u001b[Koutput\u001b[m\u001b[K, output_c * output_p * output_q);\n",
            "      |                     \u001b[01;31m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq4_conv2d_toeplitz_avx.cc:102:29:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_c\u001b[m\u001b[K’ was not declared in this scope\n",
            "  102 |   printSumOfOutputs(output, \u001b[01;31m\u001b[Koutput_c\u001b[m\u001b[K * output_p * output_q);\n",
            "      |                             \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq4_conv2d_toeplitz_avx.cc:102:40:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_p\u001b[m\u001b[K’ was not declared in this scope\n",
            "  102 |   printSumOfOutputs(output, output_c * \u001b[01;31m\u001b[Koutput_p\u001b[m\u001b[K * output_q);\n",
            "      |                                        \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq4_conv2d_toeplitz_avx.cc:102:51:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_q\u001b[m\u001b[K’ was not declared in this scope\n",
            "  102 |   printSumOfOutputs(output, output_c * output_p * \u001b[01;31m\u001b[Koutput_q\u001b[m\u001b[K);\n",
            "      |                                                   \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq4_conv2d_toeplitz_avx.cc:108:31:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktemp_output\u001b[m\u001b[K’ was not declared in this scope\n",
            "  108 |   deallocateAlignedFloatArray(\u001b[01;31m\u001b[Ktemp_output\u001b[m\u001b[K);\n",
            "      |                               \u001b[01;31m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "Error compiling q4_conv2d_toeplitz_avx.cc!\n",
            "\u001b[01m\u001b[Kq5_conv2d_toeplitz_avx_openmp.cc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kq5_conv2d_toeplitz_avx_openmp.cc:104:21:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput\u001b[m\u001b[K’ was not declared in this scope\n",
            "  104 |   printSumOfOutputs(\u001b[01;31m\u001b[Koutput\u001b[m\u001b[K, output_c * output_p * output_q);\n",
            "      |                     \u001b[01;31m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq5_conv2d_toeplitz_avx_openmp.cc:104:29:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_c\u001b[m\u001b[K’ was not declared in this scope\n",
            "  104 |   printSumOfOutputs(output, \u001b[01;31m\u001b[Koutput_c\u001b[m\u001b[K * output_p * output_q);\n",
            "      |                             \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq5_conv2d_toeplitz_avx_openmp.cc:104:40:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_p\u001b[m\u001b[K’ was not declared in this scope\n",
            "  104 |   printSumOfOutputs(output, output_c * \u001b[01;31m\u001b[Koutput_p\u001b[m\u001b[K * output_q);\n",
            "      |                                        \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq5_conv2d_toeplitz_avx_openmp.cc:104:51:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_q\u001b[m\u001b[K’ was not declared in this scope\n",
            "  104 |   printSumOfOutputs(output, output_c * output_p * \u001b[01;31m\u001b[Koutput_q\u001b[m\u001b[K);\n",
            "      |                                                   \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq5_conv2d_toeplitz_avx_openmp.cc:110:31:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktemp_output\u001b[m\u001b[K’ was not declared in this scope\n",
            "  110 |   deallocateAlignedFloatArray(\u001b[01;31m\u001b[Ktemp_output\u001b[m\u001b[K);\n",
            "      |                               \u001b[01;31m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "Error compiling q5_conv2d_toeplitz_avx_openmp.cc!\n",
            "\u001b[01m\u001b[Kq6_conv2d_toeplitz_blas.cc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kq6_conv2d_toeplitz_blas.cc:101:21:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput\u001b[m\u001b[K’ was not declared in this scope\n",
            "  101 |   printSumOfOutputs(\u001b[01;31m\u001b[Koutput\u001b[m\u001b[K, output_c * output_p * output_q);\n",
            "      |                     \u001b[01;31m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq6_conv2d_toeplitz_blas.cc:101:29:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_c\u001b[m\u001b[K’ was not declared in this scope\n",
            "  101 |   printSumOfOutputs(output, \u001b[01;31m\u001b[Koutput_c\u001b[m\u001b[K * output_p * output_q);\n",
            "      |                             \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq6_conv2d_toeplitz_blas.cc:101:40:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_p\u001b[m\u001b[K’ was not declared in this scope\n",
            "  101 |   printSumOfOutputs(output, output_c * \u001b[01;31m\u001b[Koutput_p\u001b[m\u001b[K * output_q);\n",
            "      |                                        \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq6_conv2d_toeplitz_blas.cc:101:51:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Koutput_q\u001b[m\u001b[K’ was not declared in this scope\n",
            "  101 |   printSumOfOutputs(output, output_c * output_p * \u001b[01;31m\u001b[Koutput_q\u001b[m\u001b[K);\n",
            "      |                                                   \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kq6_conv2d_toeplitz_blas.cc:107:31:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktemp_output\u001b[m\u001b[K’ was not declared in this scope\n",
            "  107 |   deallocateAlignedFloatArray(\u001b[01;31m\u001b[Ktemp_output\u001b[m\u001b[K);\n",
            "      |                               \u001b[01;31m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "Error compiling q6_conv2d_toeplitz_blas.cc!\n",
            "All files compiled successfully!\n"
          ]
        }
      ],
      "source": [
        "!bash compile.csh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0k8aaYu6zXv",
        "outputId": "3616437c-2953-408a-cae1-a0c02093bca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running q1_conv2d_naive ...\n",
            "Virtual Memory (KB): 16716\n",
            "temp: 59863496\n",
            "Time taken by function: 30232 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "-------------------------------------------\n",
            "Running q2_conv2d_toeplitz ...\n",
            "Virtual Memory (KB): 87904\n",
            "temp: 59863496\n",
            "Time taken by function: 12606 milliseconds\n",
            "Sum of all outputs: 47049932800\n",
            "-------------------------------------------\n",
            "Running q3_conv2d_toeplitz_transB ...\n",
            "run.csh: line 20: ./build/q3_conv2d_toeplitz_transB: No such file or directory\n",
            "-------------------------------------------\n",
            "Running q4_conv2d_toeplitz_avx ...\n",
            "run.csh: line 20: ./build/q4_conv2d_toeplitz_avx: No such file or directory\n",
            "-------------------------------------------\n",
            "Running q5_conv2d_toeplitz_avx_openmp ...\n",
            "run.csh: line 20: ./build/q5_conv2d_toeplitz_avx_openmp: No such file or directory\n",
            "-------------------------------------------\n",
            "Running q6_conv2d_toeplitz_blas ...\n",
            "run.csh: line 20: ./build/q6_conv2d_toeplitz_blas: No such file or directory\n",
            "-------------------------------------------\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!bash run.csh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Jeis0Q6BUO"
      },
      "source": [
        "## Compile and run your code:\n",
        "* To compile your C++ code, run command `!bash compile.csh`\n",
        "* To run your executable code, run command `!bash run.csh`\n",
        "* Check those csh files and modify accordingly when testing your code.\n",
        "* You can write and test your code on your local machine but make sure you can compile and run them on Colab. Also you should report the performance measured from Colab enverionment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNu5C22Q6BUO"
      },
      "source": [
        "## Write code and answer all questions in this notebook.\n",
        "Note that to measure the performance of each code, we want you to flush your cache. All the template files provide code that flush caches, but we need you to find the cache size of your system. Use the command below to display information about your CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OzO0J8J6BUO",
        "outputId": "16453808-d7e3-4031-b930-b4ade671ef84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:          6\n",
            "    Model:               79\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            0\n",
            "    BogoMIPS:            4399.99\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\n",
            "                         a cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscal\n",
            "                         l nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopo\n",
            "                         logy nonstop_tsc cpuid tsc_known_freq pni pclmulqdq sss\n",
            "                         e3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes \n",
            "                         xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefe\n",
            "                         tch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_ad\n",
            "                         just bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed ad\n",
            "                         x smap xsaveopt arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    256 KiB (1 instance)\n",
            "  L3:                    55 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy ba\n",
            "                         rriers only; no swapgs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBR\n",
            "                         S: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ],
      "source": [
        "!lscpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hk0XSjP9_n3"
      },
      "source": [
        "## Q1\n",
        "Implement 2D Convolution in the ``q1_conv2d_naive.cc`` using nested for loops. Assume batch size = 1, no padding and stride = 1. Check `util.h` file and understand what each function does. Use the micro `INDEX_4D_TO_1D` to help convert 4d index to 1d index. Measure and report runtime  (in milliseconds) and memory usage (in KB). Manually calculate memory usage and report. Does your calculation match with the measurement?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer to question1\n",
        "%%writefile q1_conv2d_naive.cc\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "#include \"../utils.h\"\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = (32 + 32 + 256 + 55 * 1024) * 1024;  // Sum of all cache levels\n",
        ";\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float *cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "  initializeBuffer(cacheFlush, cacheFloatSize);\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // Allocate input buffers\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float *weight = allocateAlignedFloatArray(weightSize);\n",
        "  float *input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // Initialize input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "  //initializeBuffer(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  // TODO: calculate output dimension and allocate output buffer\n",
        "  const unsigned int output_c = weight_o;\n",
        "  const unsigned int output_p = input_h - weight_r + 1;\n",
        "  const unsigned int output_q = input_w - weight_s + 1;\n",
        "  const unsigned int outputSize = output_c * output_p * output_q;\n",
        "  float *output = allocateAlignedFloatArray(outputSize);\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: use nested for loop to compute convolution\n",
        "  for (unsigned int o = 0; o < weight_o; o++)\n",
        "  {\n",
        "    for (unsigned int p = 0; p < output_p; p++)\n",
        "    {\n",
        "      for (unsigned int q = 0; q < output_q; q++)\n",
        "      {\n",
        "        float value = 0.0;\n",
        "        for (unsigned int i = 0; i < weight_i; i++)\n",
        "        {\n",
        "          for (unsigned int r = 0; r < weight_r; r++)\n",
        "          {\n",
        "            for (unsigned int s = 0; s < weight_s; s++)\n",
        "            {\n",
        "              value += input[INDEX_4D_TO_1D(0, i, p + r, q + s, 1, input_c, input_h, input_w)] * weight[INDEX_4D_TO_1D(o, i, r, s, weight_o, weight_i, weight_r, weight_s)];\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "        output[INDEX_4D_TO_1D(0, o, p, q, 1, output_c, output_p, output_q)] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  // for the runtime of this code, please measure convolution only\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // sum all outputs\n",
        "  float sum = 0.0;\n",
        "  for (int i = 0; i < 1 * output_c * output_p * output_q; i++)\n",
        "  {\n",
        "    sum += output[i];\n",
        "  }\n",
        "  cout.precision(20);\n",
        "  cout << \"Sum of all outputs: \" << sum << endl;\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  deallocateAlignedFloatArray(output);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "rUhUYNvExB58",
        "outputId": "f575f7cb-e1db-44c1-ae93-ff61f40b5af5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting q1_conv2d_naive.cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ q1_conv2d_naive.cc -o output_program"
      ],
      "metadata": {
        "id": "1xPRloCnREqp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./output_program"
      ],
      "metadata": {
        "id": "iTdlC4qzRGmK",
        "outputId": "c4cf22c6-d79f-421e-a8ea-568398ab9587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual Memory (KB): 16716\n",
            "temp: 59863496\n",
            "Time taken by function: 25683 milliseconds\n",
            "Sum of all outputs: 47243571200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBXb2fV_-YO7"
      },
      "source": [
        "## Q2\n",
        "Use Img2col algorithm to convert the input matrix and kernel matrix to toeplitz form in file ``q2_conv2d_toeplitz.cc``. Interpret input toeplitz matrix and kernel toeplitz matrix as 2d matrix, and store both of them in row major. Use nested for loops to perform matrix multiplication in `matMul` function and call it in `main` function . There is another micro `INDEX_2D_TO_1D` that helps convert 2d index to 1d index. Measure and report runtime and memory usage. Manually calculate memory usage and report. Does your calculation match with the measurement?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"\" > q2_conv2d_toeplitz.cc"
      ],
      "metadata": {
        "id": "rsDMnkiPW5RR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer to Question2\n",
        "%%writefile q2_conv2d_toeplitz.cc\n",
        "#include <immintrin.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "void im2col(const float* data_im, int channels, int height, int width,\n",
        "            int ksize, int stride, float* data_col) {\n",
        "    int output_h = (height - ksize) / stride + 1;\n",
        "    int output_w = (width - ksize) / stride + 1;\n",
        "    int col_length = channels * ksize * ksize;\n",
        "    for (int c = 0; c < col_length; ++c) {\n",
        "        int w_offset = c % ksize;\n",
        "        int h_offset = (c / ksize) % ksize;\n",
        "        int c_im = c / ksize / ksize;\n",
        "        for (int h = 0; h < output_h; ++h) {\n",
        "            for (int w = 0; w < output_w; ++w) {\n",
        "                int im_row = h_offset + h * stride;\n",
        "                int im_col = w_offset + w * stride;\n",
        "                int col_index = (c * output_h + h) * output_w + w;\n",
        "                data_col[col_index] = data_im[im_col + width * (im_row + height * c_im)];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void matMul(const float *A, const float *B, float *C, int A_rows, int A_cols,\n",
        "            int B_rows, int B_cols)\n",
        "{\n",
        "  if (A_cols != B_rows)\n",
        "  {\n",
        "    // The matrices can't be multiplied if A's number of columns\n",
        "    // isn't equal to B's number of rows.\n",
        "    throw std::invalid_argument(\n",
        "        \"Matrix dimensions mismatch for multiplication\");\n",
        "  }\n",
        "\n",
        "  // TODO: implement matmul using 3 loops\n",
        "  for (int i = 0; i < A_rows; ++i)\n",
        "  {\n",
        "    for (int j = 0; j < B_cols; ++j)\n",
        "    {\n",
        "      float sum = 0;\n",
        "      for (int k = 0; k < A_cols; ++k)\n",
        "      {\n",
        "        sum += A[INDEX_2D_TO_1D(i, k, A_rows, A_cols)] * B[INDEX_2D_TO_1D(k, j, B_rows, B_cols)];\n",
        "      }\n",
        "      C[INDEX_2D_TO_1D(i, j, A_rows, B_cols)] = sum;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = (32 + 32 + 256 + 55 * 1024) * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float *cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "  initializeBuffer(cacheFlush, cacheFloatSize);\n",
        "\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "  // dimension of the output\n",
        "  const unsigned int output_p = input_h - weight_r + 1;\n",
        "  const unsigned int output_q = input_w - weight_s + 1;\n",
        "  const unsigned int output_c = weight_o;\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float *weight = allocateAlignedFloatArray(weightSize);\n",
        "  float *input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz output and allocate buffer\n",
        "  const unsigned int output_h = input_h - weight_r + 1;\n",
        "  const unsigned int output_w = input_w - weight_s + 1;\n",
        "  const unsigned int toeplitzOutputSize = weight_o * output_h * output_w;\n",
        "  float *toeplitzOutput = allocateAlignedFloatArray(toeplitzOutputSize);\n",
        "  // TODO: calculate dimension of toeplitz input and allocate buffer\n",
        "  const unsigned int toeplitzInputRows = output_h * output_w;\n",
        "  const unsigned int toeplitzInputCols = input_c * weight_r * weight_s;\n",
        "  const unsigned int toeplitzInputSize = toeplitzInputRows * toeplitzInputCols;\n",
        "  float *toeplitzInput = allocateAlignedFloatArray(toeplitzInputSize);\n",
        "  // TODO: calculate dimension of toeplitz weight and allocate buffer\n",
        "  const unsigned int toeplitzWeightRows = weight_o;\n",
        "  const unsigned int toeplitzWeightCols = input_c * weight_r * weight_s;\n",
        "  const unsigned int toeplitzWeightSize = toeplitzWeightRows * toeplitzWeightCols;\n",
        "  float *toeplitzWeight = allocateAlignedFloatArray(toeplitzWeightSize);\n",
        "  const unsigned int temp_output_size = toeplitzInputRows * toeplitzWeightRows;\n",
        "  float *temp_output = allocateAlignedFloatArray(temp_output_size);\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "  float *output = allocateAlignedFloatArray(toeplitzOutputSize);\n",
        "  // TODO: convert matrix input & weight into toeplitz matrices\n",
        "  // toeplitz input is stored in row major and toeplitz weight is stored in row\n",
        "  // major order\n",
        "  im2col(input, input_c, input_h, input_w, weight_r, 1, toeplitzInput);\n",
        "  // Flatten the filter for Toeplitz matrix\n",
        "  for (int oc = 0; oc < weight_o; ++oc) {\n",
        "        for (int ic = 0; ic < input_c; ++ic) {\n",
        "            for (int i = 0; i < weight_r; ++i) {\n",
        "                for (int j = 0; j < weight_s; ++j) {\n",
        "                    int out_index = j + weight_s * (i + weight_r * ic);\n",
        "                    toeplitzWeight[INDEX_2D_TO_1D(oc, out_index, weight_o, input_c * weight_r * weight_s)] = weight[oc * input_c * weight_r * weight_s + ic * weight_r * weight_s + i * weight_s + j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: compute matmul between toeplitz matrices, you can create a temp\n",
        "  // buffer to store output_toeplitz measure runtime of this code only\n",
        "  // Allocate memory for temp_output using the provided function\n",
        "  matMul(toeplitzInput, toeplitzWeight, temp_output, toeplitzInputRows, toeplitzInputCols, toeplitzWeightCols, toeplitzWeightRows);\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // TODO: reformat toeplitz y into row major\n",
        "  for (int oc = 0; oc < output_c; ++oc) {\n",
        "    for (int op = 0; op < output_p; ++op) {\n",
        "        for (int oq = 0; oq < output_q; ++oq) {\n",
        "            int flat_index = oc * output_p * output_q + op * output_q + oq;\n",
        "            output[INDEX_2D_TO_1D(oc, op * output_q + oq, output_c, output_p * output_q)] = temp_output[flat_index];\n",
        "        }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printSumOfOutputs(output, output_c * output_p * output_q);\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  deallocateAlignedFloatArray(output);\n",
        "  deallocateAlignedFloatArray(temp_output);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "Z9RtL_i1lAhk",
        "outputId": "edd5e8b4-d160-4c08-deaa-be53f0d11a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting q2_conv2d_toeplitz.cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ q2_conv2d_toeplitz.cc -o output_program_02"
      ],
      "metadata": {
        "id": "7T5Ty4FlldaN"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./output_program_02"
      ],
      "metadata": {
        "id": "SaGnmJVpltUK",
        "outputId": "72888e82-5716-4ed6-8216-3a3e7fbc2ecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual Memory (KB): 87904\n",
            "temp: 59863496\n",
            "Time taken by function: 11396 milliseconds\n",
            "Sum of all outputs: 47049932800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjmFTWrK_FJU"
      },
      "source": [
        "## Q3\n",
        "For `q3_conv2d_toeplitz_tranB.cc`, repeat procedures in Q2, but store kernel toeplitz matrix in column major and modify `matMul` function accordingly. Measure and report the runtime and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"\" > q3_conv2d_toeplitz_transB.cc"
      ],
      "metadata": {
        "id": "9pnJiWtHrhMH"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer to Question3\n",
        "%%writefile q3_conv2d_toeplitz_transB.cc\n",
        "#include <immintrin.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "void im2col(const float* data_im, int channels, int height, int width,\n",
        "            int ksize, int stride, float* data_col) {\n",
        "    int output_h = (height - ksize) / stride + 1;\n",
        "    int output_w = (width - ksize) / stride + 1;\n",
        "    int col_length = channels * ksize * ksize;\n",
        "    for (int c = 0; c < col_length; ++c) {\n",
        "        int w_offset = c % ksize;\n",
        "        int h_offset = (c / ksize) % ksize;\n",
        "        int c_im = c / ksize / ksize;\n",
        "        for (int h = 0; h < output_h; ++h) {\n",
        "            for (int w = 0; w < output_w; ++w) {\n",
        "                int im_row = h_offset + h * stride;\n",
        "                int im_col = w_offset + w * stride;\n",
        "                int col_index = (c * output_h + h) * output_w + w;\n",
        "                data_col[col_index] = data_im[im_col + width * (im_row + height * c_im)];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void matMulTransB(const float* A, const float* B, float* C, int A_rows, int A_cols, int B_rows, int B_cols) {\n",
        "  if (A_cols != B_cols) {\n",
        "    throw std::invalid_argument(\"Matrix dimensions mismatch for multiplication\");\n",
        "  }\n",
        "\n",
        "  for (int i = 0; i < A_rows; ++i) {\n",
        "    for (int j = 0; j < B_rows; ++j) {\n",
        "      float sum = 0;\n",
        "      for (int k = 0; k < A_cols; ++k) {\n",
        "        sum += A[INDEX_2D_TO_1D(i, k, A_rows, A_cols)] * B[INDEX_2D_TO_1D(j, k, B_rows, B_cols)];\n",
        "      }\n",
        "      C[INDEX_2D_TO_1D(i, j, A_rows, B_rows)] = sum;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = (32 + 32 + 256 + 55 * 1024) * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float *cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "  initializeBuffer(cacheFlush, cacheFloatSize);\n",
        "\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "  // dimension of the output\n",
        "  const unsigned int output_p = input_h - weight_r + 1;\n",
        "  const unsigned int output_q = input_w - weight_s + 1;\n",
        "  const unsigned int output_c = weight_o;\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float *weight = allocateAlignedFloatArray(weightSize);\n",
        "  float *input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz output and allocate buffer\n",
        "  const unsigned int output_h = input_h - weight_r + 1;\n",
        "  const unsigned int output_w = input_w - weight_s + 1;\n",
        "  const unsigned int toeplitzOutputSize = weight_o * output_h * output_w;\n",
        "  float *toeplitzOutput = allocateAlignedFloatArray(toeplitzOutputSize);\n",
        "  // TODO: calculate dimension of toeplitz input and allocate buffer\n",
        "  const unsigned int toeplitzInputRows = output_h * output_w;\n",
        "  const unsigned int toeplitzInputCols = input_c * weight_r * weight_s;\n",
        "  const unsigned int toeplitzInputSize = toeplitzInputRows * toeplitzInputCols;\n",
        "  float *toeplitzInput = allocateAlignedFloatArray(toeplitzInputSize);\n",
        "  // TODO: calculate dimension of toeplitz weight and allocate buffer\n",
        "  const unsigned int toeplitzWeightRows = weight_o;\n",
        "  const unsigned int toeplitzWeightCols = input_c * weight_r * weight_s;\n",
        "  const unsigned int toeplitzWeightSize = toeplitzWeightRows * toeplitzWeightCols;\n",
        "  float *toeplitzWeight = allocateAlignedFloatArray(toeplitzWeightSize);\n",
        "  const unsigned int temp_output_size = toeplitzInputRows * toeplitzWeightRows;\n",
        "  float *temp_output = allocateAlignedFloatArray(temp_output_size);\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "  float *output = allocateAlignedFloatArray(toeplitzOutputSize);\n",
        "  // TODO: convert matrix input & weight into toeplitz matrices\n",
        "  // toeplitz input is stored in row major and toeplitz weight is stored in row\n",
        "  // major order\n",
        "  im2col(input, input_c, input_h, input_w, weight_r, 1, toeplitzInput);\n",
        "  // Flatten the filter for Toeplitz matrix in column major format\n",
        "  for (int oc = 0; oc < weight_o; ++oc) {\n",
        "     for (int ic = 0; ic < input_c; ++ic) {\n",
        "        for (int i = 0; i < weight_r; ++i) {\n",
        "            for (int j = 0; j < weight_s; ++j) {\n",
        "                int out_index = i + weight_r * (j + weight_s * ic);\n",
        "                toeplitzWeight[INDEX_2D_TO_1D(out_index, oc, input_c * weight_r * weight_s, weight_o)] = weight[oc * input_c * weight_r * weight_s + ic * weight_r * weight_s + i * weight_s + j];\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: compute matmul between toeplitz matrices, you can create a temp\n",
        "  // buffer to store output_toeplitz measure runtime of this code only\n",
        "  // Allocate memory for temp_output using the provided function\n",
        " matMulTransB(toeplitzInput, toeplitzWeight, temp_output, toeplitzInputRows, toeplitzInputCols, toeplitzWeightRows, toeplitzWeightCols);\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // TODO: reformat toeplitz y into row major\n",
        "  for (int oc = 0; oc < output_c; ++oc) {\n",
        "    for (int op = 0; op < output_p; ++op) {\n",
        "        for (int oq = 0; oq < output_q; ++oq) {\n",
        "            int flat_index = oc * output_p * output_q + op * output_q + oq;\n",
        "            output[INDEX_2D_TO_1D(oc, op * output_q + oq, output_c, output_p * output_q)] = temp_output[flat_index];\n",
        "        }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printSumOfOutputs(output, output_c * output_p * output_q);\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  deallocateAlignedFloatArray(output);\n",
        "  deallocateAlignedFloatArray(temp_output);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "_FIMahCv0YBI",
        "outputId": "04fcd494-ad09-4c40-d9b8-795b52f43412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting q3_conv2d_toeplitz_transB.cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ q3_conv2d_toeplitz_transB.cc -o output_program_03"
      ],
      "metadata": {
        "id": "8hVA_zdd0x8S"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./output_program_03"
      ],
      "metadata": {
        "id": "oSWHGlTo0yMv",
        "outputId": "6d11546f-6432-472e-de48-3d634a29486e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual Memory (KB): 87904\n",
            "temp: 59863496\n",
            "Time taken by function: 8456 milliseconds\n",
            "Sum of all outputs: 47961726976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5SaluD5_ydp"
      },
      "source": [
        "## Q4\n",
        "For ``q4_conv2d_toeplitz_avx.cc``, use Intel AVX (Advanced Vector Extensions) instruction set to perform matmul operation. Intel AVX instructions are Single Instruction Multiple Data (SIMD) instructions that can process 8 floating-point operands in a single instruction. Store input toeplitz matrix in row major and kernel toeplitz matrix in column major. An example of using Intel AVX is given in file ``examples/example_vectorsum_simd.cc``. Measure and report runtime and memory usage.\n",
        "\n",
        "Hint 1: Use `_mm256_add_ps` and `_mm256_mul_ps` instructions.\n",
        "\n",
        "Hint 2: If a vector is not divisible by 8, the remaining elements in the vector should not be processed by SIMD instruction. Instead, use normal scalar operations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer to Question4\n",
        "%%writefile q4_conv2d_toeplitz_avx.cc\n",
        "#include <immintrin.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "void im2col(const float* data_im, int channels, int height, int width,\n",
        "            int ksize, int stride, float* data_col) {\n",
        "    int output_h = (height - ksize) / stride + 1;\n",
        "    int output_w = (width - ksize) / stride + 1;\n",
        "    int col_length = channels * ksize * ksize;\n",
        "    for (int c = 0; c < col_length; ++c) {\n",
        "        int w_offset = c % ksize;\n",
        "        int h_offset = (c / ksize) % ksize;\n",
        "        int c_im = c / ksize / ksize;\n",
        "        for (int h = 0; h < output_h; ++h) {\n",
        "            for (int w = 0; w < output_w; ++w) {\n",
        "                int im_row = h_offset + h * stride;\n",
        "                int im_col = w_offset + w * stride;\n",
        "                int col_index = (c * output_h + h) * output_w + w;\n",
        "                data_col[col_index] = data_im[im_col + width * (im_row + height * c_im)];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void matMulAVXTransB(const float* A, const float* B, float* C, int A_rows,\n",
        "                     int A_cols, int B_rows, int B_cols) {\n",
        "    if (A_cols != B_rows) {\n",
        "        throw std::invalid_argument(\"Matrix dimensions mismatch for multiplication\");\n",
        "    }\n",
        "\n",
        "    int A_cols_divided_by_8 = A_cols - (A_cols % 8);\n",
        "    for (int i = 0; i < A_rows; ++i) {\n",
        "        for (int j = 0; j < B_cols; ++j) {\n",
        "            C[i * B_cols + j] = 0.0;\n",
        "\n",
        "            __m256 sum = _mm256_setzero_ps();\n",
        "            for (int k = 0; k < A_cols_divided_by_8; k += 8) {\n",
        "                __m256 a = _mm256_load_ps(&A[i*A_cols + k]);\n",
        "                __m256 b = _mm256_load_ps(&B[j*B_rows + k]);\n",
        "                sum = _mm256_add_ps(sum, _mm256_mul_ps(a, b));\n",
        "            }\n",
        "            float* buffer = (float*) _mm_malloc(8 * sizeof(float), 32);\n",
        "            _mm256_store_ps(buffer, sum);\n",
        "            for (int index = 0; index < 8; ++index) {\n",
        "            C[i * B_cols + j] += buffer[index];\n",
        "            }\n",
        "            for (int k = A_cols_divided_by_8; k < A_cols; ++k) {\n",
        "                C[i * B_cols + j] += A[i * A_cols + k] * B[j * B_rows + k];\n",
        "            }\n",
        "            _mm_free(buffer);\n",
        "        }\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = (32 + 32 + 256 + 55 * 1024) * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float *cacheFlush = (float*) aligned_alloc(256, (32 + 32 + 256 + 55 * 1024) * 1024);\n",
        "  initializeBuffer(cacheFlush, cacheFloatSize);\n",
        "\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "  // dimension of the output\n",
        "  const unsigned int output_p = input_h - weight_r + 1;\n",
        "  const unsigned int output_q = input_w - weight_s + 1;\n",
        "  const unsigned int output_c = weight_o;\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float *weight = (float*) aligned_alloc(256, weightSize * sizeof(float));\n",
        "  float *input = (float*) aligned_alloc(256, inputSize * sizeof(float));\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz output and allocate buffer\n",
        "  const unsigned int output_h = input_h - weight_r + 1;\n",
        "  const unsigned int output_w = input_w - weight_s + 1;\n",
        "  const unsigned int toeplitzOutputSize = weight_o * output_h * output_w;\n",
        "  float* toeplitzOutput = (float*) aligned_alloc(256, toeplitzOutputSize * sizeof(float));\n",
        "  // TODO: calculate dimension of toeplitz input and allocate buffer\n",
        "  const unsigned int toeplitzInputRows = output_h * output_w;\n",
        "  const unsigned int toeplitzInputCols = input_c * weight_r * weight_s;\n",
        "  const unsigned int toeplitzInputSize = toeplitzInputRows * toeplitzInputCols;\n",
        "  float* toeplitzInput = (float*) aligned_alloc(256, toeplitzInputSize * sizeof(float));\n",
        "  // TODO: calculate dimension of toeplitz weight and allocate buffer\n",
        "  const unsigned int toeplitzWeightRows = weight_o;\n",
        "  const unsigned int toeplitzWeightCols = input_c * weight_r * weight_s;\n",
        "  const unsigned int toeplitzWeightSize = toeplitzWeightRows * toeplitzWeightCols;\n",
        "  float* toeplitzWeight = (float*) aligned_alloc(256, toeplitzWeightSize * sizeof(float));\n",
        "  const unsigned int temp_output_size = toeplitzInputRows * toeplitzWeightRows;\n",
        "  float* temp_output = (float*) aligned_alloc(256, temp_output_size * sizeof(float));\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "  float* output = (float*) aligned_alloc(256, toeplitzOutputSize * sizeof(float));\n",
        "  // TODO: convert matrix input & weight into toeplitz matrices\n",
        "  // toeplitz input is stored in row major and toeplitz weight is stored in row\n",
        "  // major order\n",
        "  im2col(input, input_c, input_h, input_w, weight_r, 1, toeplitzInput);\n",
        "  // Flatten the filter for Toeplitz matrix in column major format\n",
        "  for (int oc = 0; oc < weight_o; ++oc) {\n",
        "     for (int ic = 0; ic < input_c; ++ic) {\n",
        "        for (int i = 0; i < weight_r; ++i) {\n",
        "            for (int j = 0; j < weight_s; ++j) {\n",
        "                int out_index = i + weight_r * (j + weight_s * ic);\n",
        "                toeplitzWeight[INDEX_2D_TO_1D(out_index, oc, input_c * weight_r * weight_s, weight_o)] = weight[oc * input_c * weight_r * weight_s + ic * weight_r * weight_s + i * weight_s + j];\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: compute matmul between toeplitz matrices, you can create a temp\n",
        "  // buffer to store output_toeplitz measure runtime of this code only\n",
        "  // Allocate memory for temp_output using the provided function\n",
        "  matMulAVXTransB(toeplitzInput, toeplitzWeight, temp_output, toeplitzInputRows, toeplitzInputCols, toeplitzWeightCols, toeplitzWeightRows);\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // TODO: reformat kernel toeplitz y into column major\n",
        "  for (int oc = 0; oc < weight_o; ++oc) {\n",
        "    for (int ic = 0; ic < input_c; ++ic) {\n",
        "        for (int i = 0; i < weight_r; ++i) {\n",
        "            for (int j = 0; j < weight_s; ++j) {\n",
        "                int out_index = j + weight_s * (i + weight_r * ic);\n",
        "                toeplitzWeight[INDEX_2D_TO_1D(oc, out_index, weight_o, input_c * weight_r * weight_s)] = weight[oc * input_c * weight_r * weight_s + ic * weight_r * weight_s + i * weight_s + j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "  printSumOfOutputs(output, output_c * output_p * output_q);\n",
        "\n",
        "  // free all allocated buffers\n",
        "  free(input);\n",
        "  free(toeplitzOutput);\n",
        "  free(toeplitzInput);\n",
        "  free(toeplitzWeight);\n",
        "  free(temp_output);\n",
        "  free(output);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "AD3ItWGnB4V6",
        "outputId": "b768f6a6-f51f-40de-8900-54eef2d8c6ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting q4_conv2d_toeplitz_avx.cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ q4_conv2d_toeplitz_avx.cc -mavx -o output_program_04"
      ],
      "metadata": {
        "id": "WNtlNnvNGYN2"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./output_program_04"
      ],
      "metadata": {
        "id": "gawNNhDsGg3A",
        "outputId": "9a7af43b-0a04-4126-8cac-3bb5cc0e3902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual Memory (KB): 87904\n",
            "temp: 59863496\n",
            "Time taken by function: 2620 milliseconds\n",
            "Sum of all outputs: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqRyp0x8A99J"
      },
      "source": [
        "## Q5\n",
        "\n",
        "### Part 1\n",
        "In file ``q5_conv2d_toeplitz_avx_openmp.cc``, futher optimize the AVX matmul operation using ``OpenMP`` library, which enables multi-threaded parallel computing automatically through a simple and flexible interface. An example of using ``OpenMP`` is given in file ``examples/example_vectorsum_simd_omd.cc``. Run the code with different number of threads. Measure and report the runtime for number of threads = [2, 4, 8].\n",
        "\n",
        "\n",
        "### Part 2 (Optional - No Credit)\n",
        "\n",
        "In file ``q5_optional_conv2d_toeplitz_avx_multi_thread.cc``, instead of using `OpenMP`, use the C++ standard `<thread>` library to implement multi-threaded AVX matmul operation. You can refer to [this video](https://youtu.be/3aqxaZsvn80?si=1QEE580e2vLmrqPO) to learn about multi threading in C++.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-_R8gm6BUQ"
      },
      "source": [
        "## Q6\n",
        "\n",
        "In file ``q6_conv2d_toeplitz_blas.cc``, use `BLAS` library to implement matmul operation. You can decide the storage format for input and kernel toeplitz matrices, and make sure you set input arguments of `cblas_sgemm` accordingly. An example is given in file ``examples/example_matmul_blas.cc``. Search online document of `cblas_sgemm` API if you are unclear about the mearning of each of its input argument. Follow the example and and finish your own code. Measure and report runtime and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer to Question6\n",
        "%%writefile q6_conv2d_toeplitz_blas.cc\n",
        "extern \"C\" {\n",
        "#include \"../cblas.h\"\n",
        "}\n",
        "#include <immintrin.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "void im2col(const float* data_im, int channels, int height, int width,\n",
        "    int ksize, int stride, float* data_col) {\n",
        "    int output_h = (height - ksize) / stride + 1;\n",
        "    int output_w = (width - ksize) / stride + 1;\n",
        "    int col_length = channels * ksize * ksize;\n",
        "    for (int c = 0; c < col_length; ++c) {\n",
        "        int w_offset = c % ksize;\n",
        "        int h_offset = (c / ksize) % ksize;\n",
        "        int c_im = c / ksize / ksize;\n",
        "        for (int h = 0; h < output_h; ++h) {\n",
        "            for (int w = 0; w < output_w; ++w) {\n",
        "                int im_row = h_offset + h * stride;\n",
        "                int im_col = w_offset + w * stride;\n",
        "                int col_index = (c * output_h + h) * output_w + w;\n",
        "                data_col[col_index] = data_im[im_col + width * (im_row + height * c_im)];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void matMulBLAS(const float* A, const float* B, float* C, int A_rows, int A_cols, int B_rows, int B_cols) {\n",
        "if (A_cols != B_rows) {\n",
        "    std::cout << \"Matrix A Columns: \" << A_cols << std::endl;\n",
        "    std::cout << \"Matrix B Rows: \" << B_rows << std::endl;\n",
        "    throw std::invalid_argument(\"Matrix dimensions mismatch for multiplication\");\n",
        "}\n",
        "    std::cout << \"Matrix A Dimensions: \" << A_rows << \"x\" << A_cols << std::endl;\n",
        "    std::cout << \"Matrix B Dimensions: \" << B_rows << \"x\" << B_cols << std::endl;\n",
        "    int M = A_rows;\n",
        "    int N = B_cols;\n",
        "    int K = A_cols;\n",
        "\n",
        "    float alpha = 1.0f;\n",
        "    float beta = 0.0f;\n",
        "    cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans,\n",
        "                M, N, K, alpha,\n",
        "                A, K,\n",
        "                B, N, beta,\n",
        "                C, N);\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = (32 + 32 + 256 + 55 * 1024) * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float *cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "  initializeBuffer(cacheFlush, cacheFloatSize);\n",
        "\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "  // dimension of the output\n",
        "  const unsigned int output_p = input_h - weight_r + 1;\n",
        "  const unsigned int output_q = input_w - weight_s + 1;\n",
        "  const unsigned int output_c = weight_o;\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float *weight = allocateAlignedFloatArray(weightSize);\n",
        "  float *input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz output and allocate buffer\n",
        "  const unsigned int output_h = input_h - weight_r + 1;\n",
        "  const unsigned int output_w = input_w - weight_s + 1;\n",
        "  const unsigned int toeplitzOutputSize = weight_o * output_h * output_w;\n",
        "  float *toeplitzOutput = allocateAlignedFloatArray(toeplitzOutputSize);\n",
        "  // TODO: calculate dimension of toeplitz input and allocate buffer\n",
        "  const unsigned int toeplitzInputRows = output_h * output_w;\n",
        "  const unsigned int toeplitzInputCols = input_c * weight_r * weight_s;\n",
        "  const unsigned int toeplitzInputSize = toeplitzInputRows * toeplitzInputCols;\n",
        "  float *toeplitzInput = allocateAlignedFloatArray(toeplitzInputSize);\n",
        "  // TODO: calculate dimension of toeplitz weight and allocate buffer\n",
        "  const unsigned int toeplitzWeightRows = weight_o;\n",
        "  const unsigned int toeplitzWeightCols = input_c * weight_r * weight_s;\n",
        "  const unsigned int toeplitzWeightSize = toeplitzWeightRows * toeplitzWeightCols;\n",
        "  float *toeplitzWeight = allocateAlignedFloatArray(toeplitzWeightSize);\n",
        "  const unsigned int temp_output_size = toeplitzInputRows * toeplitzWeightRows;\n",
        "  float *temp_output = allocateAlignedFloatArray(temp_output_size);\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "  float *output = allocateAlignedFloatArray(toeplitzOutputSize);\n",
        "  // TODO: convert matrix input & weight into toeplitz matrices\n",
        "  // toeplitz input is stored in row major and toeplitz weight is stored in row\n",
        "  // major order\n",
        "  im2col(input, input_c, input_h, input_w, weight_r, 1, toeplitzInput);\n",
        "  // Flatten the filter for Toeplitz matrix in column major format\n",
        "  for (int oc = 0; oc < weight_o; ++oc) {\n",
        "    for (int ic = 0; ic < input_c; ++ic) {\n",
        "        for (int i = 0; i < weight_r; ++i) {\n",
        "            for (int j = 0; j < weight_s; ++j) {\n",
        "                int out_index = i + weight_r * (j + weight_s * ic);\n",
        "                toeplitzWeight[INDEX_2D_TO_1D(oc, out_index, weight_o, input_c * weight_r * weight_s)] =\n",
        "                    weight[oc * input_c * weight_r * weight_s + ic * weight_r * weight_s + i * weight_s + j];\n",
        "            }\n",
        "         }\n",
        "      }\n",
        "   }\n",
        "\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: compute matmul between toeplitz matrices, you can create a temp\n",
        "  // buffer to store output_toeplitz measure runtime of this code only\n",
        "  // Allocate memory for temp_output using the provided function\n",
        "  std::cout << \"Matrix A Rows: \" << toeplitzInputRows << std::endl;\n",
        "  std::cout << \"Matrix B Columns: \" << toeplitzWeightCols << std::endl;\n",
        "  matMulBLAS(toeplitzInput, toeplitzWeight, temp_output, toeplitzInputRows, toeplitzInputCols, toeplitzWeightRows, toeplitzWeightCols);\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // TODO: reformat toeplitz y into row major\n",
        "  for (int oc = 0; oc < output_c; ++oc) {\n",
        "    for (int op = 0; op < output_p; ++op) {\n",
        "        for (int oq = 0; oq < output_q; ++oq) {\n",
        "            int flat_index = oc * output_p * output_q + op * output_q + oq;\n",
        "            output[INDEX_2D_TO_1D(oc, op * output_q + oq, output_c, output_p * output_q)] = temp_output[flat_index];\n",
        "        }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printSumOfOutputs(output, output_c * output_p * output_q);\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  deallocateAlignedFloatArray(output);\n",
        "  deallocateAlignedFloatArray(temp_output);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "3D5qqBhCaqc_",
        "outputId": "f59783c3-299f-4b61-b6e0-3914572e447c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting q6_conv2d_toeplitz_blas.cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ q6_conv2d_toeplitz_blas.cc -lcblas -o output_program_06"
      ],
      "metadata": {
        "id": "xPToVgK7btaM"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./output_program_06"
      ],
      "metadata": {
        "id": "oER55VrFb1qL",
        "outputId": "e3d4d23c-9c6c-459b-aca0-f0c98c06f8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual Memory (KB): 87904\n",
            "temp: 59863496\n",
            "Matrix A Rows: 15876\n",
            "Matrix B Columns: 1152\n",
            "Matrix A Columns: 1152\n",
            "Matrix B Rows: 128\n",
            "terminate called after throwing an instance of 'std::invalid_argument'\n",
            "  what():  Matrix dimensions mismatch for multiplication\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CVxmA8VB2n5"
      },
      "source": [
        "## Q7  \n",
        "\n",
        "### Part 1\n",
        "\n",
        "\n",
        "Analyze the performance differences between Q1-Q6. Explain what constitutes the performance difference between each implementations and why `BLAS` library is super fast.\n",
        "\n",
        "### Part 2: Using Google Benchmark (Optional - No Credit)\n",
        "As an optional practice, you can measure the runtime of each matrix calculation method more accurately using Google benchmark.\n",
        "\n",
        "A better way to measure performance of a funcion in C++ is to use Google Benchmark. You can refer to [this video](https://youtu.be/9VKR8u9odrA?si=xSInuzT5uMBOKAbP) to familiarize yourself with this package.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m7NCnmR6BUQ"
      },
      "source": [
        "## Upload files to GitHub\n",
        "Make sure upload your final C++ code and this IPython notebook to GitHub Repo either mannully or through git commands."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}