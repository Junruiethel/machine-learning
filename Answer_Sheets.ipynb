{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junruiethel/machine-learning/blob/main/Answer_Sheets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EE 599 Reading Assignment \\# 2:\n"
      ],
      "metadata": {
        "id": "JA0rETRQZBON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1: (1 Point)\n",
        "\n",
        "**Disallowed** list:\n",
        "- You **MAY NOT** collaborate with anyone else on this assignment. This means you cannot talk to anyone else about the assignment until after deadline.\n",
        "- You **MAY NOT** use ChatGPT and services like that\n",
        "\n",
        "**Allowed** list:\n",
        "- Notes including any slides from the class\n",
        "- The textbooks\n",
        "- The given paper"
      ],
      "metadata": {
        "id": "s-HWWv6kaxrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A1:\n",
        "\n",
        "I affirm I have read these exam rules and will follow them. Failure to do so may subject me to sanctions including an F in the course.\n",
        "\n",
        "**Type your full name to affirm you have read the above statement:**\n"
      ],
      "metadata": {
        "id": "Pz0uD-nHbHuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Junrui Wan"
      ],
      "metadata": {
        "id": "8D8TdCz_Y6_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2 (99 Points):\n",
        "\n",
        "Use this [paper](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf) as your guideline on how to read papers in general.\n",
        "\n",
        "For this assignment, please write a summary of this paper:\n",
        "\n",
        "\"In-Datacenter Performance Analysis of a Tensor Processing Unit\"\n",
        "\n",
        "Complete info:\n",
        "\n",
        "Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates et al. \"In-datacenter performance analysis of a tensor processing unit.\" In Proceedings of the 44th annual international symposium on computer architecture, pp. 1-12. 2017.\n",
        "\n",
        "You can download it from [here](https://arxiv.org/pdf/1704.04760.pdf?mod=article_inline)."
      ],
      "metadata": {
        "id": "IzxAsb2ErvaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## Q2.1 Summary (19 Points):\n",
        "\n",
        "Summarize the main objectives and contributions of the paper."
      ],
      "metadata": {
        "id": "XzNWLVCLbcjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A2.1:\n",
        "Based on the provided segments of the document titled \"TPU article.pdf\", here's a summary of the main objectives and contributions:\n",
        "\n",
        "### Objectives:\n",
        "1. **Evaluate Tensor Processing Units (TPUs)**: The paper aims to provide an analysis of Google's TPUs, custom-built hardware accelerators designed specifically for machine learning tasks.\n",
        "2. **Comparison with Contemporary CPUs and GPUs**: The document seeks to compare the performance of TPUs with the capabilities of popular CPUs (like Haswell) and GPUs (like K80) in the context of both training and inference tasks for neural networks.\n",
        "3. **Study the Significance of Latency**: The paper underscores the importance of latency in inference applications, especially those which are user-facing and need to meet 99th-percentile response-time bounds.\n",
        "\n",
        "### Contributions:\n",
        "1. **TPUs and Matrix Multiply Unit**: The success of the TPU is attributed to its large matrix multiply unit, which is not exceedingly large. This unit, along with substantial software-controlled on-chip memory, helps in achieving performance boosts.\n",
        "2. **Comparison Results**: The TPU outperforms the K80 GPU and Haswell CPU significantly in terms of speed and energy efficiency. When it comes to inference, the TPU performs about 15 times as fast as the K80 GPU and achieves a performance/Watt advantage of up to 29 times. In comparison to the Haswell CPU, the ratios are 29 and 83 respectively. If the TPU were redesigned using 2015 GPU memory, it would perform two to three times faster, boasting a performance/Watt advantage nearly 70 times over the K80 and 200 times over Haswell.\n",
        "3. **Adaptability and TensorFlow**: The TPU's flexibility ensures it can handle neural networks from both 2017 and 2013. The use of 8-bit integers by the quantized applications, and the fact that applications were designed using TensorFlow, allowed for easy porting without needing to rewrite for the distinct TPU hardware. This showcases the TPU's adaptability to various neural network architectures and tasks.\n",
        "4. **Domain-Specific Architectures**: Given the significant differences in performance metrics between TPUs and general-purpose hardware like CPUs and GPUs, the paper suggests that TPUs might pave the way for future domain-specific architectures, raising the standards even higher.\n",
        "5. **Acknowledgements**: The paper acknowledges the efforts of many in the development, deployment, and evaluation of TPUs, indicating that it's a collaborative effort involving hardware design, software integration, and large-scale deployment.\n",
        "\n",
        "Overall, the paper emphasizes the potential and benefits of domain-specific hardware like TPUs in the landscape of neural network training and inference, highlighting their superior performance and energy efficiency compared to general-purpose computing hardware."
      ],
      "metadata": {
        "id": "0j5A4_Zfbpra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.2 Comprehension (10 Points):\n",
        "- What is a Tensor Processing Unit (TPU) and how does it differ from traditional CPUs and GPUs in terms of design and purpose?\n",
        "- Why is there a need for specialized hardware like TPUs in modern data centers?"
      ],
      "metadata": {
        "id": "IE2n5UQHbuZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A2.2:\n",
        "\n",
        "\n",
        "1.\n",
        "\n",
        "   A Tensor Processing Unit (TPU) is a custom-built ASIC designed to accelerate machine learning workloads. It's specifically tailored for Google's Machine Learning software, TensorFlow, and is utilized to run neural network computations. The design philosophy and purpose behind TPUs differ markedly from traditional CPUs and GPUs:\n",
        "   \n",
        "   - **Design**: The TPU leverages the significant reduction in energy and area of 8-bit integer systolic matrix multipliers over 32-bit floating-point datapaths of a typical GPU (like the K80). This allows the TPU to pack significantly more MACs (multiply-accumulate operations) and more on-chip memory while using less than half the power of a traditional GPU. Moreover, it has a large matrix multiplication unit and substantial software-controlled on-chip memory.\n",
        "   - **Purpose**: While CPUs and GPUs are general-purpose processors suitable for a broad range of tasks, TPUs are domain-specific architectures designed exclusively to deliver high performance and efficiency for neural network inference and training tasks.\n",
        "\n",
        "2.  The need for specialized hardware like TPUs in modern data centers arises from the following reasons:\n",
        "\n",
        "   - **Performance Demands**: With the advent and rising popularity of deep learning and neural networks, there's a substantial computational demand. The specialized design of TPUs allows them to execute these computations much more efficiently than general-purpose CPUs or GPUs.\n",
        "   \n",
        "   - **Cost-Effectiveness**: TPUs can deliver high performance with low utilization of resources. The article introduces the “Cornucopia Corollary” to Amdahl’s Law, which suggests that low utilization of a vast, cheap resource can still yield high, cost-effective performance.\n",
        "   \n",
        "   - **Energy Efficiency**: TPUs achieve an order-of-magnitude reduction in energy compared to typical GPUs for specific machine learning tasks. This makes them especially valuable in large data centers where power consumption is a critical concern.\n",
        "   \n",
        "   - **Workload Specificity**: The article indicates that certain neural network types dominate in datacenter workloads, with CNNs making up just 5% and suggesting more focus might be needed on MLPs and LSTMs. Specialized hardware like TPUs can be tailored to cater to the dominant workloads more efficiently.\n",
        "   \n",
        "   - **Latency Concerns**: Inference applications often have strict response-time bounds since they are part of user-facing applications. TPUs are designed with these latency concerns in mind, allowing them to meet strict response-time deadlines better than some GPUs which might emphasize throughput over latency.\n"
      ],
      "metadata": {
        "id": "9O7k2DbXcNp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.3 Technical Deep Dive (15 Points):\n",
        "- Describe the architecture of the TPU. How is it optimized for machine learning workloads?\n",
        "- How does the memory hierarchy of the TPU differ from traditional processors, and why is this significant for tensor computations?\n",
        "- What are systolic arrays, and why are they used in TPUs?"
      ],
      "metadata": {
        "id": "PnzcxvhWckY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.3:\n",
        "Certainly, based on the provided excerpts from the article:\n",
        "\n",
        "1. **Describe the architecture of the TPU. How is it optimized for machine learning workloads?**\n",
        "   \n",
        "   The Tensor Processing Unit (TPU) has an architecture that heavily leans towards accelerating machine learning tasks. A few standout features include:\n",
        "   \n",
        "   - **Matrix Multiply Unit**: The TPU's design prominently features a large matrix multiply unit that can handle a significant number of operations efficiently. Matrix multiplications are a staple in deep learning, especially in feedforward and convolutional layers of neural networks.\n",
        "   \n",
        "   - **On-Chip Memory**: The TPU is equipped with 28 MiB of software-controlled on-chip memory, which is more than three times that of the referenced K80 GPU. Having a larger on-chip memory is essential for improving the operational intensity of applications, allowing them to utilize the MACs more fully. This is critical for the rapid processing of large neural network models.\n",
        "   \n",
        "   - **8-bit Integer Systolic Matrix Multipliers**: Instead of the 32-bit floating-point datapaths present in GPUs like the K80, the TPU uses 8-bit integer systolic matrix multipliers. This design choice offers an order-of-magnitude reduction in energy and area, enabling the TPU to pack significantly more MACs while consuming less power.\n",
        "\n",
        "2. **How does the memory hierarchy of the TPU differ from traditional processors, and why is this significant for tensor computations?**\n",
        "\n",
        "   While the exact memory hierarchy of the TPU isn't detailed in the provided excerpts, there are some notable points about its memory:\n",
        "   \n",
        "   - **Large On-Chip Memory**: The TPU has 28 MiB of on-chip memory. This is significantly larger than many traditional processors, especially when compared to the 8 MiB on the referenced K80 GPU. On-chip memory is much faster than off-chip memory, and this abundance allows for rapid access to the data required for tensor computations, reducing the need for slow memory fetches from off-chip sources.\n",
        "   \n",
        "   - **Optimized for Operational Intensity**: The large on-chip memory boosts the operational intensity of applications, which means that more operations can be performed for every byte fetched from memory. In the context of tensor computations, which often involve repeated operations on the same data, this means that more computations can be performed without constantly fetching new data.\n",
        "   \n",
        "3. **What are systolic arrays, and why are they used in TPUs?**\n",
        "\n",
        "   Systolic arrays are a type of digital computing architecture. They consist of a grid of processors that compute data and pass it to their neighbors in a rhythmic, wave-like manner (akin to a heartbeat, hence the name \"systolic\"). In the context of the TPU, 8-bit integer systolic matrix multipliers are employed.\n",
        "   \n",
        "   The reason systolic arrays are used in TPUs is due to their efficiency in performing matrix operations, which are fundamental in deep learning computations. Each processor in the systolic array can perform a multiply-accumulate (MAC) operation and then pass its result to the next processor, allowing for parallel processing of matrix elements. This design choice ensures high throughput and energy efficiency, making it well-suited for the intensive computations required in machine learning workloads."
      ],
      "metadata": {
        "id": "sv3rFNjpcuID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.4 Evaluation (25 Points):\n",
        "- How does the TPU's performance compare to contemporary CPUs and GPUs for machine learning tasks?\n",
        "- What benchmarks or workloads were used to evaluate the TPU's performance in the datacenter?\n",
        "- Discuss the TPU's performance per watt. Why is energy efficiency crucial in datacenter environments?\n",
        "- What types of machine learning models and tasks are best suited for TPUs?\n",
        "- How have TPUs impacted the training times of large-scale models at Google?"
      ],
      "metadata": {
        "id": "HRBvFutTc2uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.4:\n",
        "Based on the provided excerpts from the article, here are the answers to your questions:\n",
        "\n",
        "1. **How does the TPU's performance compare to contemporary CPUs and GPUs for machine learning tasks?**\n",
        "   - The TPU was shown to outperform contemporary hardware for certain tasks. Specifically, when compared to the K80 GPU die, the TPU ran programs using the TensorFlow framework approximately 15 times faster. Against the Haswell CPU die, the performance advantage was around 29 times. These advantages are significant, especially when considering the computational demands of machine learning tasks.\n",
        "\n",
        "2. **What benchmarks or workloads were used to evaluate the TPU's performance in the datacenter?**\n",
        "   - The exact benchmarks were not detailed in the provided excerpts. However, the article did mention \"representative NN workload for our datacenters\" which indicates a set of neural network tasks that represent typical workloads in Google's data centers. The emphasis seems to be on real-world, practical machine learning tasks rather than purely synthetic benchmarks.\n",
        "\n",
        "3. **Discuss the TPU's performance per watt. Why is energy efficiency crucial in datacenter environments?**\n",
        "   - The TPU demonstrated a significant performance per watt advantage. Compared to the K80 GPU, the TPU had a performance/Watt advantage of 29 times. Against the Haswell CPU, this advantage was even greater at 83 times. Energy efficiency is critical in datacenter environments because data centers operate at a massive scale, and even minor inefficiencies in power usage can result in significant costs. Furthermore, excessive power consumption generates heat, which then requires additional power for cooling. Therefore, energy-efficient hardware like the TPU not only reduces electricity bills but also minimizes the infrastructure and operational costs associated with cooling.\n",
        "\n",
        "4. **What types of machine learning models and tasks are best suited for TPUs?**\n",
        "   - While the article emphasizes the TPU's proficiency in running neural network tasks, it also points out that despite a recent emphasis on CNNs (Convolutional Neural Networks) in the architecture community, they make up only about 5% of the representative NN workload for Google's datacenters. This suggests that other models like MLPs (Multi-Layer Perceptrons) and LSTMs (Long Short-Term Memory networks) are also significant and might be well-suited for TPUs.\n",
        "\n",
        "5. **How have TPUs impacted the training times of large-scale models at Google?**\n",
        "   - The provided excerpts focus primarily on the TPU's capabilities for inference rather than training. However, the inherent advantages in speed and efficiency would logically benefit training times for large-scale models. Faster matrix multiplications and greater on-chip memory would likely accelerate the forward and backward passes in neural network training. But the exact impact of TPUs on training times at Google, based on the given excerpts, is not explicitly detailed."
      ],
      "metadata": {
        "id": "xKUL8nhTdNLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.5 Contextual Understanding (10 Points):\n",
        "- What are some of the challenges faced in deploying TPUs in datacenters?\n",
        "- Are there specific machine learning tasks or models that TPUs might struggle with compared to other hardware?\n",
        "\n"
      ],
      "metadata": {
        "id": "Fp_FV2bzdV4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.5:\n",
        "\n",
        "1. **What are some of the challenges faced in deploying TPUs in datacenters?**\n",
        "   - The article mentions a few challenges related to TPUs in the context of datacenters. One is that the TPU lives on an I/O bus and has a relatively limited memory bandwidth. This limitation restricts the utilization of the TPU, especially as four of the six neural network (NN) applications mentioned are memory-bound. Another challenge is ensuring that TPUs meet the strict response-time deadlines of inference applications. Many of these applications are user-facing, and thus any latency could degrade the user experience.\n",
        "\n",
        "2. **Are there specific machine learning tasks or models that TPUs might struggle with compared to other hardware?**\n",
        "   - The article points out that, while there has been a recent emphasis on Convolutional Neural Networks (CNNs) in the architecture community, they constitute only about 5% of the representative NN workload for Google's datacenters. This suggests that the majority of the workloads are made up of other types of neural network architectures, like Multi-Layer Perceptrons (MLPs) and Long Short-Term Memory networks (LSTMs). Therefore, while the TPU might be optimized for certain NN tasks, it's possible that it might not be as optimized for others, especially if those tasks were more predominant in architectures like CPUs or GPUs before TPUs were introduced. Additionally, the article doesn't explicitly state that TPUs struggle with any particular task, but it does emphasize the importance of ensuring that TPUs remain versatile enough to handle the evolving landscape of machine learning models."
      ],
      "metadata": {
        "id": "OvpxYhvNdg18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.6 Discussion and Critique (10 points):\n",
        "- What are the strengths and weaknesses of the paper's methodology and analysis?\n",
        "- Are there any potential biases or assumptions in the paper that you disagree with or find questionable?\n"
      ],
      "metadata": {
        "id": "cxUpnGmtdjfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.6:\n",
        "Based on the provided excerpts from the article, here's an evaluation of the paper's methodology, analysis, and potential biases or assumptions:\n",
        "\n",
        "**Strengths of the Methodology and Analysis:**\n",
        "1. **Comprehensive Analysis:** The paper offers a thorough examination of the TPU, comparing its performance with contemporary CPUs and GPUs. This holistic approach provides readers with a clear perspective on the TPU's advantages.\n",
        "2. **Real-World Context:** The authors evaluated the TPU's performance based on actual workloads in Google's datacenters, making the findings practical and immediately relevant.\n",
        "3. **Diverse Benchmarks:** The benchmarks used to evaluate the TPU are based on a diverse set of neural network architectures, which provides a comprehensive view of its capabilities across various tasks.\n",
        "4. **Energy Efficiency Evaluation:** The paper doesn't limit its analysis to raw performance. It also looks into performance per watt, which is a critical metric in datacenter environments.\n",
        "\n",
        "**Weaknesses of the Methodology and Analysis:**\n",
        "1. **Potential for Bias:** Since the paper was written by researchers from Google (the company behind TPUs) and aims to highlight the TPU's advantages, there might be an inherent bias in how the data and results are presented.\n",
        "2. **Limited to Specific Benchmarks:** While the benchmarks used are diverse, they represent a subset of all possible machine learning tasks. It would be interesting to see how TPUs fare on a wider variety of tasks.\n",
        "3. **Reliance on Specific Technologies:** The paper's emphasis on TensorFlow as the primary framework could be viewed as a limitation, as results might vary with other frameworks.\n",
        "\n",
        "**Potential Biases or Assumptions:**\n",
        "1. **TensorFlow Emphasis:** The paper emphasizes that the TPU is optimized for TensorFlow. While TensorFlow is popular, there are many other machine learning frameworks. The TPU's performance might be different on non-TensorFlow tasks.\n",
        "2. **Future Projections:** The article mentions potential improvements in future TPUs using \"circa 2015 GPU memory.\" Such projections could be optimistic and might not account for other advancements in CPU or GPU technology during the same period.\n",
        "3. **Emphasis on Specific Neural Network Architectures:** While the article does mention a variety of neural networks, there's a significant emphasis on CNNs, despite them constituting only about 5% of the representative workload in their datacenters. This focus might overshadow other critical architectures like MLPs and LSTMs.\n",
        "\n"
      ],
      "metadata": {
        "id": "cjAQy1qLdvZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.7 Reflection (10 Points):\n",
        "- How do TPUs fit into the larger trend of custom hardware accelerators for machine learning, such as FPGAs and ASICs?\n",
        "\n",
        "- Discuss the trade-offs between general-purpose hardware (like CPUs) and specialized hardware (like TPUs) in the context of evolving machine learning workloads.\n"
      ],
      "metadata": {
        "id": "hSMCnPg8d_0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.7:\n",
        "\n",
        "1. **How do TPUs fit into the larger trend of custom hardware accelerators for machine learning, such as FPGAs and ASICs?**\n",
        "\n",
        "   - **Customization for Specific Workloads:** TPUs represent a broader trend in the hardware industry where there's a move towards domain-specific architectures. These architectures are optimized for particular tasks, in this case, machine learning. FPGAs (Field-Programmable Gate Arrays) and ASICs (Application-Specific Integrated Circuits) are other examples of this trend. FPGAs offer reconfigurability, allowing them to be tailored for specific tasks, while ASICs, like TPUs, are designed for a specific application from the outset.\n",
        "   \n",
        "   - **Performance and Efficiency:** The article underscores the TPU's advantages in performance, particularly in machine learning tasks, over general-purpose processors like CPUs. The TPU's order-of-magnitude reduction in energy and area, thanks to its 8-bit integer systolic matrix multipliers over the 32-bit floating-point datapaths of GPUs, is emblematic of how custom accelerators can achieve significant efficiency gains.\n",
        "\n",
        "2. **Discuss the trade-offs between general-purpose hardware (like CPUs) and specialized hardware (like TPUs) in the context of evolving machine learning workloads:**\n",
        "\n",
        "   - **Flexibility vs. Optimization:** General-purpose hardware like CPUs is designed to handle a wide range of tasks. This flexibility means that they can run various applications decently well but may not be optimized for any particular task. In contrast, specialized hardware like TPUs is designed with a specific workload in mind. As a result, they can achieve impressive performance gains in those tasks but might not be as versatile outside their domain.\n",
        "   \n",
        "   - **Cost and Development Time:** Designing and manufacturing specialized hardware can be expensive and time-consuming. Every time there's a significant shift in machine learning models or algorithms, specialized hardware might need adjustments. CPUs, being general-purpose, can adapt to these shifts through software changes without requiring new hardware designs.\n",
        "   \n",
        "   - **Performance and Efficiency:** For tasks they're designed for, specialized hardware can offer significant advantages in terms of raw performance and energy efficiency. The TPU's performance per watt advantage over both the K80 GPU and Haswell CPU highlighted in the article is a testament to this.\n",
        "   \n",
        "   - **Workload Predictability:** Using specialized hardware assumes a certain level of predictability in workloads. If machine learning models and algorithms evolve rapidly, investing heavily in specialized hardware could be risky. On the other hand, CPUs offer a safer bet as they can handle a broader range of tasks, albeit potentially less efficiently.\n",
        "\n",
        "   - **Integration with Existing Infrastructure:** Deploying specialized hardware in environments dominated by general-purpose machines might pose integration challenges. There could be compatibility issues, software stack adjustments, or the need for specialized knowledge and training.\n"
      ],
      "metadata": {
        "id": "nBtgEgPXeNR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Turn in your reading assignment by saving this answer sheet back to the Github repository."
      ],
      "metadata": {
        "id": "mnRyd9Iie6Dz"
      }
    }
  ]
}